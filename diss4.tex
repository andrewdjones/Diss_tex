\chapter{Clustering, similarity, and learned harmonic categories}
%Intro: claim I'll show how to extract functional syntactic clusters without assuming time scales or pitch relationships
In the last chapter, I illustrated one way to read different types of temporal progression out of statistics for hand-assembled chord categories that I labeled $ii$ or $V$ or $I$.  Given a group of ``origin chords" captured by one of those category labels, I isolated expected ``destination chords" and investigated the differences between their temporal probability distributions.  By the end of the chapter, I was in a position to ask: can we extract this type of information without knowing in advance which origin chords, destination chords, and types of temporal progression are important?  Put in computational terms, can we bootstrap both the chord categories and the temporal progression regimes?

The methods necessary to answer questions of this nature lie at the heart of the project laid out in these pages.  In chapter one, I attempted to show a circularity embedded in analytical-reductive descriptions of musical syntax employed by Lerdahl \& Jackendoff, while chapter two responded to the circularity Robert Gjerdingen identifies in a range of corpus analytical projects.  If harmonic syntax can be extracted from a corpus of performance in a non-circular way, I claim that it should be \emph{learned}, in a computational sense.  This chapter will show what a data-driven, minimally-circular pathway to syntax extraction might look like, while the concluding chapter to follow will question the benefits (and possibility) of syntactic models of a variety of types.

Running PCA on the full temporal probability distribution set for a hand-assembled origin chord category ($ii$, in the preceding chapter's case) implied that the most salient temporal progression regimes can be identified through unsupervised machine learning.  This chapter extends that observation to the formation of origin chord categories, applying similarity measures to a set of origin chord properties with careful choices of metric and basis.  Ultimately, I show that performing flattened agglomerative hierarchical clustering on PCA-reduced temporal probability distributions produces categories of chord similarity which reflect the temporal properties of the data in a robust and sensitive way, reproducing traditional notions of chord similarity and providing a framework for supportable claims of harmonic syntax.  Moreover, supervised classifier models based on the resulting category assignments indicate the potential for exploring and tagging low-probability chords.

\section{Temporal Probability Distributions: From curves to matrices}
%This step isn't formally necessary(?), but it'll really help music people, I think
%Basically just explain how you can stick all those distributions into ordered matrices
The previous chapter explained how each origin chord can be associated with a probability versus time curve for each possible destination chord it ``progresses" to within the subsequent 5 seconds.  In the case of $ii$, plotting the myriad possible destination chord curves yielded the messy Figure~\ref{ii_messy}.  Identifying which destination chords appear in similar ways visually, curve by curve, is a daunting task for such a figure, and the process of finding what other non-$ii$ origin chords have sets of destination curves similar to those of Figure~\ref{ii_messy} would be an exceptionally difficult task for a human analyst.  Aggregating data for a huge number of origin chords, however, renders the problem well-suited for clustering algorithms.

Taking a step back from the visualized destination chord probability curves, we might describe each origin chord as a site of temporal property binding.
%IF Kockelman invoked: To borrow from the ontological descriptions of objects employed by Kockelman, ``an object is just a bundle of features (or projected propensities to exhibit certain features...) relative to which an agent's sensations and instigations make sense (given some process of selection)."\footnote{Kockelmman , 2012 p.\ 18.}
Each origin chord is a harmonic object after which other harmonic objects tend to sound after particular time delays.  One way to \emph{represent} an origin chord, then, in either a semiotic or mathematical sense, is with the set of temporal statistics for what destination chords appear afterwards.\footnote{Semiotically, the temporal progression matrices are complicated signs indexing particular origin chord objects, and the semiotic relations connecting these properties, objects, clustering algorithms, and cluster assignments fit well into the ontological framework given in Kockelman 2012.  Mathematically, I do not here mean that the temporal progression matrices of this representation require any particular group properties, and I leave open what kinds of vector space would most productively ground the representation.  But the point of mathematical representation theory is clearly applicable here -- and it may even be possible to combine the predictive power of several origin chord matrices through appropriately-formalized matrix operations.  This question remains for future work.}  Just as the behavior of the $ii$ chord could be described with probability curves corresponding to the $n$ possible destination chords succeeding it, any origin chord can be associated with an $n$-by-$t$ matrix, where each of the $n$ rows is the temporal probability distribution for a destination chord over the $t$ time windows following the appearance of the origin chord.  Since every origin chord has such a matrix,\footnote{Formally, some of these matrices might be empty; if a rare chord appeared only at the very end of performances, it would be associated with no destination chords at all.  In such cases, the chord can still be described by its (empty) destination chord matrix -- the probability of any other chord at all subsequent times is zero.} we can compare origin chords to one another based purely on how similar these matrices are to one another -- requiring no knowledge at all about what pitches or scale degrees are in the origin chord, whether they form a consonant sonority, or what the ``expected" functional behavior of the chord might be.

%FIGURE NEEDED
%include plot comparison of a bunch of TPDs for one origin chord and a schematic matrix of the same data?

Associating chord identity with temporal probability distributions invokes a radical kind of behavioralism: to find what kinds of origin chords are similar to one another, corpus analysis allows us to step away from expectations regarding the pitch similarity of the chords entirely.  As far as the algorithmic work is concerned, the chords could be named with labels like $[0,2,5]$, or $[\hat{1},\hat{2},\hat{4}]$, or $(C,D,F)$, or Lucy, or any other label such that distinct scale degree sets receive distinct names, and the same statistical identity would result.

%Behavioralism (if that's a word) over pitch similarity!  Destabilizes ontology (though maybe leave that out, for now)
Characterizing origin chords entirely in terms of their temporal successors sidesteps a variety of questions regarding what descriptive basis is best.  To determine whether a chord like $[2,7,11]$ behaves more like the upper extension of $iii^7$ or the lower extension of $V^7$, we need not compare the rootedness or pitch set overlap between $[2,7,11]$ and any idealized chord labeled with a roman numeral; instead, we might look for other origin chord types that have temporal probability distributions most similar to those of $[2,7,11]$.  Automated clustering algorithms based on the temporal statistics of chord succession accomplish precisely this task at scale and in an unsupervised manner.  In the YJaMP corpus, the statistics support the claim that the pianists I recorded most often use $[2,7,11]$ in ways similar to other ``tonic-type" chords like $iii^7$, $I^6$, and $vi^7$.

\section{Progression Matrix Similarity Measures}
%Motivate matrix ordering, truncation, and temporal basis reduction
Comparing temporal probability distribution matrices requires constructing a consistent metric for quantifying the similarity of any two origin chords.  As the temporal statistics calculated from the YJaMP corpus are finely-grained and sensitive to noise, several steps must be taken to decrease noise and increase the resulting clustering algorithm's chances of discriminating between chords of analytical interest.

%Truncation and ordering
Calculated directly from the corpus, the matrices have differing sizes.  Very uncommon chords which only occur near the end of a few performances might have only a handful of distinct destination chords which ever succeed them, producing matrices with only a few rows; at the other extreme, the most common chords (like tonics) might be followed by more than 1,000 distinct chords across their many appearances in the corpus.  Additionally, some matrices are quite sparse, including nearly-empty rows for many destination chords which only appear once or twice across the entire corpus.  Truncating the size of the tracked chord lexicon and imposing a consistent row ordering across all the resulting origin chord matrices mitigate both problems.

\begin{figure}
	\centering
	\includegraphics[width=6in]{lexicon_zipfian.png}
	\caption{The unigram probabilities for each of the 1850 distinct scale-degree sets of cardinality three or greater in the YJaMP corpus.  The scale-degree sets are ordered by descending unigram probability.  After around 200 scale-degree sets, the share of overall unigram probability captured by each new marginal set falls below $0.1\%$.}
	\label{lexicon}
\end{figure}

%Truncation and Zipf
Figure~\ref{lexicon} shows the overall unigram distribution for each of the 1850 distinct scale-degree sets of cardinality three or greater.  Like the unigram corpora of many language-like formal systems, it bears resemblance to a Zipfian distribution, where the unigram probability is inversely proportional to each chord's unigram frequency rank.\footnote{Cite some generic Zipf linguistic studies.}  For YJaMP, the distribution of Figure~\ref{lexicon} actually slopes off somewhat more slowly than a true Zipf distribution, with the leftmost part of the curve resembling $\frac{1}{r^s}$ for $0.5 < s < 1$.  For quasi-Zipfian distributions, it is common to truncate the size of the lexicon at some rank indicative of diminishing marginal probability gain. For a na\"{i}ve model consisting of only the most probable chord in the corpus -- that is, a model which predicts that every chord observed ought to be the most common chord in the corpus -- adding a second chord to the model nearly doubles the portion of the corpus data it captures.  Instead of describing the observed chord correctly only 2\% of the time, the model can now describe nearly 4\% of the observations in the corpus.  For a 200 chord model, adding one additional chord only captures at most an additional $0.1\%$.  I deem such an increase in model accuracy unworthy of the substantial increase in complexity.\footnote{Zipfian studies (and corpus-driven models in general) spend considerable time fighting over whether or not given distributions contain ``elbows," where a clear change in the first derivative of the curve signals a justifiable alphabet truncation size.  YJaMP's unigram distribution provides no obvious elbow, but the choice of precise alphabet size is a low-stakes game; choosing a number of chords slightly below or somewhat above 200 changes the ultimate category assignments made by the model by a small amount, and other alphabet size choices are defensible.}

%Manhattan metric and PCA justification
With a 200-chord lexical restriction in place, each origin chord can then be associated with an identically-sized 200 by 100 matrix; the 200 rows correspond to the destination chord probability distributions for the 200 most frequent chords in the corpus, in rank order, and the 100 columns correspond to 100 consecutive 50-millisecond time windows.  For the sake of consistency, ``empty rows" (consisting entirely of zeroes) appear for destination chords which never follow a given origin chord within five seconds.  Identically-sized matrices can be compared element-by-element, dramatically simplifying the construction of similarity metrics.  The Manhattan (or ``Taxicab") distance, a special case of the Minkowski distance metric, sums the difference between corresponding entries in two vectors or arrays.  For vectors $x$ and $y$ with $n$ dimensions each, the Manhattan distance $d_1$ is given by
\begin{eqnarray*}
x &=& (x_1,x_2,...,x_n) \\
y &=& (y_1,y_2,...,y_n) \\
d_1(x,y) &=& \sum_{i=1}^n \lvert x_i - y_i \rvert
\end{eqnarray*}
The ``Manhattan" and ``Taxicab" monikers arise from a common geometric interpretation of this distance: for the case of two-dimensional vectors $x$ and $y$, the metric is exactly analogous to the distance traveled by a taxicab between any two points on the square-block grid of Manhattan.  If no diagonal avenues are allowed, a taxi can only travel along roads which intersect at right angles; accordingly, the driving distance between any two points on the grid can be calculated from the sum of the east-west blocks and north-south blocks separating the origin and destination points.  The Manhattan metric is easily adapted to the matrix comparison task at hand -- each destination chord probability distribution is a 100-dimensional vector, and the Manhattan distance between two such vectors is the sum of the differences between each of their corresponding elements.  If each destination chord vector were viewed as the coordinates of a point in a 100-dimensional geometric space, the resulting Manhattan distance would be the number of ``blocks" traveled by a taxicab moving in one dimension at a time from one point to another.

A na\"{i}ve, element-by-element Manhattan comparison between two temporal probability distribution matrices, however, implies an identical weighting for probability differences in each time window.  While the predictive and functional properties of chords presumably diminish over time, the contributions of background noise and random variation between destination chord distributions will not.  Consider a limiting case, where the temporal probability distributions for destination chords are 1,000-dimensional vectors tracking the appearance of destination chords as late as 50 seconds after a given origin chord.  At times far after an origin chord, the destination chord distributions will almost certainly reflect the background unigram probabilities plus a contribution from small, random fluctuations uncorrelated to a particular origin chord or category.  A Manhattan metric applied to vectors in this high-dimensional space will tally increasing contributions from the random fluctuations, proportionally, drowning out the patterns in phonetic or syntactic progressions at shorter time scales.

%PCA saves the day
Just as it did in Chapter 3, PCA allows the extraction of time scales over which chords experience the highest variance with few assumptions regarding which time scales are too short or too long.  Each origin chord is associated with a matrix of destination chords, and each destination chord is represented by a row of 100 coordinates, each of which indicates the probability of the destination chord's appearance a given number of time windows after the origin chord.  But these 100 coordinates are not the only way to represent the destination chord's probability variance over time.  For each origin chord, running PCA on the set of 100-dimensional destination chords produces a set of principal components, 100-dimensional vectors themselves, capturing the combinations of time windows which account for the greatest amount of variance in the destination chord distributions.  In a way, the principal components provide ready-made templates designed to capture the most common temporal differences between kinds of destination chord behavior.  Destination chord probability distributions can then be written with a much smaller number of coordinates while losing as little accuracy as possible.  Instead of using coordinates to say ``This destination chord appears with 10\% probability at $t=1$, and 5\% probability at $t=2$, and ...", the new coordinate system might be translated to say ``This destination chord's probability curve looks like $1.2$ times the curve given by the first principal component, plus $2$ times the curve given by the second component, plus..."

In this way, each destination chord probability distribution -- each row of the origin chord's associated temporal matrix -- undergoes a basis transformation, a change in its method of coordinate description.  As described in Chapter 3, a PCA algorithm calculates the components themselves by examining coordinate variance across the full set of destination chords of a given matrix, providing one optimized set of principal components per origin chord with which to re-describe them.  And for most origin chords, especially those with high unigram probabilities, the PCA components resemble the phonetic, syntactic, and background components found for $ii$'s destination chords in Chapter 3.

Figure~\ref{PCA_examples} shows four sets of principal components capturing most of the variance in destination chord temporal probability distributions for four different origin chords.  In each case, I have retained the first three principal components.  For nearly all of the 200 most probable chords, three components are enough to capture at least 75\% of the variance in destination chord distributions; in many cases, three components capture more than 90\%.  In each plot of Figure~\ref{PCA_examples}, the components are color-coded in order of descending variance capture: the first component is blue, and typically captures more than 50\% of the variance, while the second and third components are green and red, respectively.  The horizontal axis tracks 50ms time windows, while the vertical axis indicates each component's probability loading per time window.  If an origin chord matrix contained a destination chord with a probability distribution that happened to look exactly like the first principal component, that distribution could be described in this new basis with the coordinates $(1,0,0)$ -- one multiple of the first component, and no contributions from either other component distribution.

%figure: PCA components for 4 top-200 origin chords to explain temporal basis reduction and component alignment
\begin{figure}
	\centering
	\includegraphics[width=6in]{top200_PCA.png}
	\caption{The first three principal components capturing the most temporal probability variance for four selected origin chords.  For each origin chord, the components provide a three-dimensional reduction of the 100-dimensional destination chord distributions.  Each destination chord's temporal probability distribution can then be written with only three coordinates -- the loadings for each principal component.}
	\label{PCA_examples}
\end{figure}

The top left plot in Figure~\ref{PCA_examples}, calculated from the origin chord $[0,3,5,10]$, shows principal components which directly correspond to the intuitions developed in Chapter 3 regarding temporal progression regimes.  The first (blue) component correlates high initial probability variance shortly after the origin chord with a decrease in variance over time; high coordinate values along this component capture chord behavior characteristic of the phonetic temporal regime, in which origin chords tend to progress to voice-leading neighbor destination chords with similar traditional harmonic functions over short time scales.  Compared to a randomly-chosen destination chord, a phonetic neighbor with a high PCA component 1 score would likely display a large deviation from unigram probability at short time scales, and the deviation in probability would trail off as time after the origin chord increases.  The second (green) component correlates initial probability suppression with a gradual ascent to a stable, non-zero probability value at later times, a combination well-suited to the probabilistic variance of chords common to the local key context but comparatively unlikely to occur as voice-leading neighbors to $[0,3,5,10]$.  The more complex third (red) component implies the most subtle type of temporal progression, yielding a template for destination chords which are suppressed during the phonetic regime but have higher probabilities during a finite time span afterward.  I suggested in the previous chapter that this kind of behavior captures a possible translation of ``syntactic progressions," in traditional theory parlance, where chords unlikely to serve the same local function as the origin chord are more highly probable shortly thereafter than in the long term.  Combinations of these components render the status of many destination chords ambiguous, but this simplified explanation serves to indicate the connection between principal components and destination chord probability distributions in broad strokes.

The components for the other origin chords of Figure~\ref{PCA_examples} serve to block over-interpretation of the salient features of $[0,3,5,10]$.  In each case, component one matches phonetic-scale probability expectations, but one or both of the other components appear quite different.  The nature of this difference arises from PCA itself.  If each component provides a new basis direction along which destination chords are likely to show great variance, the orientation of any particular destination chord's variance along that direction is not reflected in the principal component.  Each component captures not just one kind of probabilistic behavior, but \emph{two}: if a destination chord distribution exactly resembling the first component can be assigned PCA-basis coordinates $(1,0,0)$, a destination chord distribution showing exactly the opposite behavior -- strongly suppressed at short distances, asymptotically increasing over long durations -- could be captured by the coordinates $(-1,0,0)$.  The same component would exactly capture the probability fluctuations of the destination chord, but the relevant coordinate would carry a negative sign.

Figure~\ref{PCA_examples} thus serves as an encoded set of instructions for how to find particular kinds of chords.  To find destination chords after $[0,3,5,10]$ matching intuitive expectations for phonetic, background, and syntactic-regime progressions, the analyst should examine chords with high, positive coordinate scores along each of the three principal components.  For $[0,3,5,7,10]$, the PCA components for which appear in the top right portion of Figure~\ref{PCA_examples}, the same is true up to a correction of sign: destination chords with phonetic and syntactic regime behavior will be written with high, positive coordinates along PCA components one and three, but background-type destination chords will be written with \emph{negative} scores along the direction of PCA component two.  In other words, component two for $[0,3,5,7,10]$ resembles the mirror image of component two for $[0,3,5,10]$, its multiple by negative one, so background destination chords with positive coordinates along component two for $[0,3,5,10]$ will have similar probability distributions to destination chords with negative components along component two for $[0,3,5,7,10]$.  The same sign change describes the component difference for the bottom-left plot of Figure~\ref{PCA_examples}, $[2,5,7,11]$; like $[0,3,5,7,10]$, its second component resembles the mirror image (or negative multiple) of the second component of $[0,3,5,10]$.

The bottom-right plot, $[0,4,7,9,11]$, displays components with similar contours to the other three plots, but on a compressed time-scale.  The rate of decrease in component one and initial increases for components two and three occur much closer to the origin chord than the corresponding components for the other plotted origin chords.  This likely reflects the decreased predictive power of a tonic chord.  Within a few time windows after the appearance of $[0,4,7,9,11]$, component one indicates that voice-leading neighbors still occur with greatly increased probability, but the overall probability variance after 20 windows or so is quite small.  This provides a graphical translation of received harmonic wisdom: the appearance of a $I$ or $vi$ chord tells the analyst very little about what chords likely appear in the future, as nearly any key-appropriate chord can follow tonic chords in a syntactic phrase.

The principal components for a large number of the top 200 most probable chords in the YJaMP corpus afford observations of this nature to such an extent that the intuitive interpretation of basis components can be standardized and automated.  To see if a destination chord shows the same kind of probability fluctuations after two different origin chords, an algorithm can compare the destination chord's PCA-basis coordinates up to an overall alignment of signs.  A metric comparing two origin chord matrices, then, needs only to align the corresponding PCA components by multiplying one or more columns by negative one to achieve an imposed, standard alignment: component one starts with positive value, and components two and three start with negative values.  This alignment is arbitrary, but the resulting standardization comes at no real cost.

As a result, I compare origin chords by imposing a Manhattan metric on matrices consisting of the destination chord temporal probability distributions for the 200 most probable chords in the corpus written in a systematically-aligned, PCA-reduced basis.  Each origin chord corresponds to a matrix with 200 rows and three columns, and I employ a dissimilarity ``distance" between two origin chords defined as the sum of the absolute differences between corresponding elements in their respective matrices.  In more formal notation, the distance $d$ between chords $\alpha$ and $\beta$ is given by

\begin{equation}
d(\alpha, \beta) = \sum_{i=1}^{200} \sum_{j=1}^3 \lvert \alpha_{ij} - \beta_{ij} \rvert
\end{equation}

This distance metric remains agnostic about the pitch structures of the origin and destination chords, and it provides a (dis)similarity metric based purely on progression statistics; a larger Manhattan distance $d$ between two matrices $\alpha$ and $\beta$ correlates with a sense of increased difference between the progression behavior of the corresponding origin chords.  As the next section will show, a metric of this kind allows clustering algorithms to group origin chords together based on similarity in their overall progression tendencies, yielding a rough approximation of chord categories assembled from behavioral statistics.

\section{Clustering methods}
%with Manhattan metric in hand, explain at least flat agglomerative clustering for (functional?) categories
%go into detail about agglomerative hierarchical clustering on the topN-filtered, PCA-transformed matrices
Clustering algorithms, in their simplest form, pursue a single goal: given an array of objects described in some coordinate basis, group the objects into categories where the members of each category are ``closer" (or ``more similar") to one another than they are to members of the other categories.  Aside from the input objects, described with some consistent representation scheme, most clustering algorithms require the analyst to supply two further input choices -- a similarity metric for comparing objects to one another, and a way to determine the boundaries of each cluster.  If the input objects are represented in a basis that lends itself to a geometric interpretation (like spatial coordinates), the process of drawing these cluster boundaries may be interpreted as outlining shapes or surfaces containing the objects.  The resulting points in each category would be found close to one another, geometrically speaking -- ``clustered" together.

%hierarchical clustering in general; greedy algorithms
Hierarchical clustering algorithms approach the very complex problem of how to partition a large number of data points into clusters by splitting it into a series of simpler problems.  Starting from either of two limiting cases -- either there is only one giant cluster, into which all data points fall, or every point is its own cluster -- hierarchical clustering first either splits the giant cluster into two smaller clusters (the former, ``divisive" clustering case) or merges an individual point cluster with its closest metric-determined neighbor (the latter, ``agglomerative"  method).  As a result, the algorithm replaces the na\"{i}ve limiting case with a slightly more nuanced approximate clustering.  Repeating the split or merge step many times, hierarchical clustering gradually produces an approximation of the optimal clustering.  For each step, hierarchical clustering algorithms perform what computer scientists refer to as ``greedy" optimization, using all available dissimilarity information provided by the metric to make the best possible local decision without any consideration of its future effects; strict hierarchical clustering makes no attempt to guess in advance what cluster assignments many successive splits/merges will produce.  Due to their computationally efficient short-sightedness, greedy hierarchical clustering algorithms can occasionally get off track, producing final clusterings which poorly capture the intuitive structure of the data.

%bad results from divisive clustering?  Agglom works well; explain it
With no method for planning the end results of the greedy optimization, divisive hierarchical clustering algorithms start with a trickier task than agglomerative ones: there are many possible ways to split a large data set into two clusters, and the immediate differences between initial splits may be small.  For a noisy data set, however, many initially-productive splittings can turn out to produce ill effects in the final clustering, as data points comparatively close together might be placed into different clusters early on to optimize cluster-level criteria.  Agglomerative hierarchical clustering approaches the problem from the opposite direction.  If divisive clustering is a ``top-down" process, requiring a heuristic for how to split clusters most effectively, agglomerative clustering is a ``bottom-up" procedure, where each merge step uses the metric to determine which points 9or groups of points) are closest together.  Closely-related objects are less likely to end up placed far apart, but another kind of judgment call is still required: a linkage criterion indicating how to calculate the distance between a single data point and a cluster of arbitrary size.  If a merge operation places a point (or many points) into a cluster, it must evaluate which cluster would be the best, ``closest" assignment.  Several ways of specifying this linkage criterion are possible, but ``complete" or ``maximum" linkage provides the best results for clustering the YJaMP temporal probability matrices.\footnote{Two types of sub-optimal behavior accrue as a result of other linkage criteria: either category assignments violate a large number of syntactic expectations, or the resulting clusters differ radically in size -- one cluster might have 100 origin chords, and a dozen other clusters might contain a handful of chords each.}

Maximum linkage describes inter-cluster distance as the \emph{largest} distance between any two points in the respective clusters.  Put formally, the distance between two chord clusters $A$ and $B$ ($d_l$) can be calculated from the Manhattan-metric distances ($d_m$) between the chords in each cluster:
\begin{equation}
\label{eq:d_l}
d_l(A,B) = \max \{ d_m(c_1,c_2) \vert c_1 \in A, c_2 \in B \}
\end{equation}
Each merge step adds a single point to a cluster or combines two clusters in order to optimize the linkage criteria $d_l$.  After many such merge steps, agglomerative hierarchical clustering produces cluster assignments for the full range of observations -- in YJaMP's case, the top 200 most probable origin chords.

%now describe the results
%First, what the dendrogram even means
Running agglomerative hierarchical clustering of this kind with the sklearn (``scikit-learn") Python package produces the dendrogram given in Figure~\ref{Dend_complete}.  The 200 most unigram-probable chords are listed at the bottom of the figure in an order automatically chosen to keep cluster members adjacent to their nearest neighbors.  Each bracket on the dendrogram represents a merge operation, and the vertical-axis height of each bracket reflects the distance between its two associated ``children."  At the bottom of the figure, origin chords connected by a bracket constitute merges which associate a single chord (or ``leaf," in dendrogram parlance) with the chord deemed most similar to it -- that is, with the chord which minimizes $d_l$ in Equation~\ref{eq:d_l} above.  Higher up in the ``tree," branchings indicate merge steps where at least one of the clusters contains multiple chords.  Hierarchical clustering provides multiple levels of similarity ranging from point-to-point up to broad cluster-to-cluster comparisons.  In YJaMP's terms, this range of groupings indexes both similarity between individual chords (near the bottom of the tree) and a functional or modal organization of the origin chords into a small number of categories (near the top of the tree).

The color coded portion of the tree in Figure~\ref{Dend_complete} provides rough category assignments for the clusters.  The decision where to ``cut" the dendrogram into large-scale category clusters is comparatively arbitrary, and I impose it here via the Python package SciPy's \emph{fcluster} function, which color codes clusters based on a supplied inter-cluster distance, a particular choice of vertical-axis inter-cluster distance at which to identify categories.  I set that threshold high enough reduce the number of clusters as much as possible while minimizing the extent to which clusters containing functionally-different chords are merged.  As this is a heuristic decision, other choices of cut point are clearly defensible, and imposing them merely requires modifying the color-coding on Figure~\ref{Dend_complete}, choosing instead to categorize clusters at a slightly higher or lower location on the dendrogram.  As it stands, Figure~\ref{Dend_complete} color codes 59 potential clusters, 28 of which contain more than one chord.  The full dendrogram is difficult to read, so a series of subplots appear below.

%the big agglom clus dendrogram, at least three subplots thereof with annotation and explanation
\begin{figure}
	\centering
	\includegraphics[width=6in]{Dendrogram_complete.png}
	\caption{The full dendrogram for agglomerative hierarchical clustering of the top 200 most frequent chords in the YJaMP corpus.  Chord-to-chord distances (or dissimilarities) are calculated from PCA-reduced temporal probability distribution matrices for each chord.}
	\label{Dend_complete}
\end{figure}

Figure~\ref{Dend_sub1} shows several chord clusters capturing pre-existing harmonic expectations for normative jazz substitutions.  At left, the green and red clusters contain scale degree sets readily interpretable as varieties of $ii$ and $IV$ triads, seventh chords, and their tonally-appropriate supersets.  The green cluster groups $[2,5,9]$, the $ii$ triad in a local key, with $[0,2,5,9]$, a fully-voiced $ii^7$, and $[0,2,5,7,9]$, which could be interpreted as a $ii^{7}$ with an added 11th.  The adjacency of these three scale degree sets in the dendrogram indicates close similarity in their progression matrices, and the shortest brackets, like the one connecting $[2,5,9]$ and $[0,2,5,9]$, correspond to the closest merges.  The red cluster contains $[0,4,5,9]$, a fully-voiced $IV^7$ in the local key, as well as $[4,5,9]$, a root-third-seventh $IV^7$ voicing, and $[0,5,9]$, the $IV$ triad.  But it also captures scale-degree sets we might describe as ambiguously-rooted $ii-IV$ blends, like $[0,2,4,5,9]$, or as related scale segments, like $[2,4,5,9]$ and $[0,2,4,5]$.\footnote{Both of the latter examples, like all the scale-degree sets on the dendrogram, might be voiced in a variety of ways, so interpretive caution is advised.  If $[0,2,4,5]$ were voiced in the smallest possible span (say, $[C4,D4,E4,F4]$) it would indeed resemble a scale segment -- but if it were instead voiced as $[D4,F4,C5,E5]$, an analyst might label it as a $ii^9$ chord.}   Here, we might interpret the red and green clusters as predominant or subdominant functions, and we can see from Figure~\ref{Dend_complete} that the merge operation one level above the color coding of each of these clusters groups them together; the red and green clusters are more similar to each other than they are to the other clusters produced by the agglomerative algorithm.

At the far right of Figure~\ref{Dend_sub1}, black brackets delineate a cluster quite different from the red or green clusters.  Here, dominant-function scale-degree sets appear in proximity to one another, including $[2,5,11]$, the $vii^{\circ}$ triad, $[2,5,7,11]$, a fully-voiced $V^7$ chord, and $[4,5,7,11]$, which might be identified as a $V^7$ chord with an added 13.  This comparatively small cluster consists of chords with a rather constrained syntactic function.  In contrast, the large yellow cluster in the middle of Figure~\ref{Dend_sub1} contains major-key ``tonic" type chords and scale segments, broadly construed.  It includes tonic representatives of $iii$ ($[2,4,7]$), $vi$ ([$0,4,9]$ and $[0,4,7,9]$), and $I$ ($[0,4,7]$).  The function of each individual chord in the yellow cluster is harder to pin down (what is a $[4,5,7]$ chord?), but the members of the cluster have progression statistics more similar to one another -- and to major tonics without a leading tone in them -- than to members of other clusters.

On the smallest scale, the agglomerative clustering describes $[0,4,7]$ and $[0,4,7,9]$ as closest-neighbors.  This reproduces an old jazz-analytical truism regarding stable tonics, but it does so in a way which avoids the usual complications of roman numeral analysis.  Jazz texts written by Mehegan, Levine, Terefenko, and others describe $[0,4,7,9]$ as either a $VI^7$ chord or as a $I^6$ chord, depending on its inversional voicing; with a $\hat{6}$ in the bass, the sonority tends to receive the former roman numeral, while a chord featuring $\hat{6}$ in the top voice receives the latter.  The agglomerative clustering of Figure~\ref{Dend_sub1} knows nothing about the ``root ambiguity" of $[0,4,7,9]$ -- nor does it know that the chord shares three of its scale degrees with $[0,4,7]$.  The clustering places them adjacent to one another because their temporal progression statistics are extremely similar -- they ``behave" the same way.  This tonic ``way of behaving" can evidently be expanded to instances of $[2,7,11]$, the dissonance-free $V$ triad, and various diatonic scale segments. 

\begin{figure}
	\centering
	\includegraphics[width=6in]{Dend_ii_IV_I_vi_V.png}
	\caption{Close-up of Figure~\ref{Dend_complete}.  Color-coded clusters here capture categories corresponding to traditional scale degree set descriptions of $ii$, $IV$, $I$, $vi$, and $V$.  Note that the clustering algorithm knows only the behavior of the chords -- not their pitch class content.}
	\label{Dend_sub1}
\end{figure}
%iv appears in both of the below.  Is this because iv behaves differently in major vs. minor key contexts?
\begin{figure}
	\centering
	\includegraphics[width=6in]{Dend_iv_I7_iii.png}
	\caption{Close-up of Figure~\ref{Dend_complete}.  Color-coded clusters here capture categories corresponding to traditional scale degree set descriptions of $iv$, $I^7$, and $iii$.  Note that the clustering algorithm knows only the behavior of the chords -- not their pitch class content.}
	\label{Dend_sub2}
\end{figure}

Figure~\ref{Dend_sub2} contains a similar range of functional clusters.  At right are several clusters containing the leading tone ($[11]$ or $\hat{7}$).  The yellow cluster contains varieties of $iii^7$, like $[2,4,7,11]$ and $[4,7,11]$, as well as pitch class sets we might describe as root-ambiguous, or both $V$-like and $iii$-like.  The agglomerative clustering here sidesteps whether $[2,4,7,9,11]$ should be described as a $V$ triad with added sixth and ninth (less likely, due to the missing chordal seventh, but logically permissible) or as a $iii^{11}$; of importance to the clustering is that the scale-degree set behaves in a way very similar to the other chords in the cluster, several of which resemble traditional $iii$ chords.  The purple and cyan clusters on the right hand side of Figure~\ref{Dend_sub2} also contain tonic-type chords.  The majority of the purple curve consists of $I^7$ voicings, while the chords of the cyan cluster typically feature the addition of $\hat{6}$ (i.e., the purple cluster contains $[0,4,7,11]$, while the cyan cluster contains $[0,4,7,9,11]$).  Both the purple and cyan clusters separate the voice-leading behavior of major tonics containing leading tones from Figure~\ref{Dend_sub1}'s cluster of leading-tone-free tonics, and all three of the of the clusters at right on Figure~\ref{Dend_sub2} are similar enough to merge at the next branching point up the hierarchical tree.

Figure~\ref{Dend_sub3} shows a variety clusters suggestive of minor keys and ``flat-side" harmony.  In the cyan cluster near the middle, $[0,3,7]$, $[0,3,10]$, and $[0,3,7,10]$ appear adjacent to one another -- the three most common scale-degree expressions of the minor tonic chord.  The remainder of the cyan cluster indicates that the addition of scale degree $\hat{4}$ (in chromatic semitonal notation, $[5]$) to minor tonic chords leaves their progression statistics relatively unchanged.  While $[0,3,7]$, $[0,3,10]$, and $[0,3,7,10]$ fall in the left cyan ``subcluster," $[0,3,5,7]$, $[3,5,10]$, and $[0,3,5,7,10]$ all fall in the right.  This clustering appears to support the ``extended harmony" dictum that perfect 11ths may be added to minor chords without significantly altering their functional properties.  The purple cluster to the right contains many instances of minor tonic scale degrees $\hat{1}$, $\flat\hat{3}$, $\hat{5}$, and $\flat\hat{7}$ ($[0,3,7,$ and $10]$), but with the frequent addition of $\flat\hat{6}$ ($[8]$).  This includes chords that might carry roman numeral label $\flat VI$, like $[0,7,8]$ and $[0,3,7,8]$, as well as more ambiguous collections of minor key scale degrees, like $[7,8,10]$ and $[2,7,8]$.  This cluster's composition does not necessarily indicate that the chords appear exclusively in minor keys; rather, it demonstrates that they progress the same way over time, whether as modal mixture in major key contexts, flat-side modulatory passages, or as diatonic minor harmonies.

%Weird clusters
Aside from recognizably predominant, dominant, and tonic clusters in major and minor modes, the agglomerative clustering of Figure~\ref{Dend_complete} provides both a framework for categorizing systematically-used chords of lower probability and an indication of where the limits of cluster interpretation might lie.  The cyan cluster on the left hand side of Figure~\ref{Dend_sub1} contains a collection of chords bearing family resemblance, where no single scale degree or root occurs across all members of the cluster.  Most of the chords contain scale degree $\flat\hat{7}$, whether as part of a $v$ (like $[5,7,10]$) or a scale-degree set potentially identifiable as $\flat VII$ (like $[0,2,5,10]$), and each chord contains at least two scale degrees in common with its nearest neighbors.

%Green cluster of Figure~\ref{Dend_sub2}.  Has a lot of [8] in it?  Some bVII and v?  Possible junk drawer?
The green cluster at the left hand side of Figure~\ref{Dend_sub2} elevates the ambiguity of traditional interpretation to levels which cast doubt on the utility of the cluster.  Here, scale degree sets corresponding to $v$ ($[2,7,10]$) and $\flat VII$ ($[2,5,10]$) share a large cluster with root-third-seventh voicings $[3,5,8]$ and $[3,5,9]$.  The latter might be interpreted as a $V^7 / \flat VII$, and several other flat-side chords relative to that key area also appear (like $[3,7,10]$), but the cluster otherwise bears some resemblance to a ``junk drawer," a cluster into which the algorithm dumps noisy chords with complex behavior.

None of the chords in this green cluster are among the top 100 most unigram-probable chords except $[2,7,10]$, $[2,5,10]$, $[3,5,8]$, and $[3,7,10]$, a fact which reflects the increasing difficulty of clustering lower-probability chords in an intuitively (or syntactically) meaningful way.  When the agglomerative hierarchical clustering algorithm is given a large number of low-probability chords to tag with cluster labels, the perceived syntactic consistency of the clusters begins to break down.  Under such conditions, investigations into the potential behavior of low-probability chords benefit from \emph{supervised} machine learning classifier methods, where the initial clustering of high-probability chords is taken as a given, and new chords are categorized with respect to the initial framework. 

\begin{figure}
	\centering
	\includegraphics[width=6in]{Dend_iv_bVI_i.png}
	\caption{Close-up of Figure~\ref{Dend_complete}.  Color-coded clusters here capture categories corresponding to traditional scale degree set descriptions of $iv$, $\flat VI$, and $i$.  Note that the clustering algorithm knows only the behavior of the chords -- not their pitch class content.}
	\label{Dend_sub3}
\end{figure}

\section{Generalized harmonic syntax: categories and classifications}
%Lay out the general problem: want to get categories without noise -- especially so we can see useful behavior -- but then also use those categories for classifying noisier states
%Note that various types of machine learning algorithm can place lower-P chords into the flat agglomerative clustering classes; don't come down on one firm algorithm.
%MAYBE: Then note that training on high-P chords allows us to see some of what categories "do" (cat-to-cat transitions?)
Given a set of meaningful category assignments for high-probability chords, as introduced in the previous section, lower-probability chords may be classified into these categories by machine learning algorithms trained on the set of high-probability category assignments.  Supervised machine learning algorithms take a predictive stance with respect to ``new" observations, using patterns gleaned from known classifications to guess what tag should be assigned to a previously unseen observation.  Different specific algorithms accomplish the task in different ways, but each ``learns" from a training set -- high-probability chords and their cluster assignments, in this case -- and then attempts to extrapolate that learning to make predictions about sensible class assignments for elements of a testing set -- here, lower-probability chords not captured by the initial clustering.

Two important kinds of optimization confront any analyst choosing from the wide variety of supervised classifiers.  First, the value of a given classifier is typically assessed by examining how well it performs on a subset of the data for which the desired category assignments are known.\footnote{Give a source for cross-validation?}  In the case of YJaMP's syntactic clustering, this cross-validation scheme involves separating the flat clustering itself, in which all of the 200 most probable chords receive cluster assignments, into distinct training and testing sets.  If a machine learning algorithm is trained on a random selection of 80\% of the top 200 chords, comparing their temporal probability matrices to their cluster assignments in order to generate a predictive model mapping the former to the latter, it can then be tested on the remaining 20\% , chords which the model has never seen.  Since the flat clustering provides category assignments for these chords -- assignments the machine learning model did not see or use in its predictions -- the accuracy of the model's predictions for the ``held out" 20\% can be calculated by comparing the predicted cluster assignments to the real, pre-existing assignments made by the flat clustering algorithm.  Moreover, this validation process can be automated and repeated with new choices of 80/20 splits, decreasing the importance of any particular random selection within the training set.  The average accuracy across many such splits provides a measure of how well a particular class of supervised machine learning classifier performs -- and thus how well it should be trusted to predict cluster assignments for the unknown chords outside the top 200 most probable.

%Do I need to define accuracy/precision/recall?

Such cross-validation applies to any supervised classifier, providing a metric with which to choose between different algorithms.  Each algorithm, however, can be fine-tuned in a number of ways; most depend on several model parameters set \emph{a priori}, the effect of which on the model's predictive power may be difficult to predict.  This second kind of optimization, in which the analyst attempts to choose the best parameters for a given model, can make use of cross-validation as a tool for assessment -- but the cross-validation must be performed for each plausible combination of model input parameters, or at least some meaningful subset of that parameter space.

Fortunately, scikit-learn provides ready-made Python methods for both repeated, automated cross-validation and optimization over model parameters.  With the flat cluster assignments as tags for each of the 200 most probable chords in YJaMP, I employ two main scikit-learn methods: \emph{train\_test\_split} randomly divides the clustering into an $80\%$ development/training set and a $20\%$ testing set, and \emph{GridSearchCV} checks a wide variety of model parameters via cross-validation on the training set.  Given a ``grid" consisting of many combinations of model parameters, GridSearchCV uses every possible combination of those input parameters to build a model based on the development set.  It employs a multi-fold cross-validation scheme, where the development set is divided into four different parts, and each part is sequentially ``held out" as an internal testing set.  Based on each parametric model's performance in the cross-validation, GridSearchCV chooses the set of parameters most likely to yield a high-accuracy model on the final testing set.  Put another way, the model selection process follows a series of steps:

\begin{enumerate}
	\item Divide the overall flat clustering into a development set ($80\%$) and a testing set ($20\%$).
	\item For a given supervised classifier type, choose a grid of parameters over which to optimize.
	\item For each combination of parameters, divide the development set into four equal and randomly selected ``folds" for cross-validation.
	\item Treat each of these folds as a testing set, sequentially; when fold 1 is the testing set, train the model on folds 2-4; when fold 2 is the testing set, train the model on folds 1, 3 and 4; etc.
	\item Calculate the average accuracy score across all folds for a given combination of model parameters.  Select the parameters which yield the best score.
	\item With model and parameters in hand, train the selected classifier on the full development set and test it on the originally held-out testing set.
\end{enumerate}

\subsection{How can chords be classified into categories?}


\subsection{What do categories do?}
From origin chord matrices compared using a PCA-reduced manhattan metric within an agglomerative clustering scheme (flattened), can tally category temporal progression distributions precisely analogous to those for $ii$ in chapter 3, but without assuming the categories in advance.

Can still consider both phonetic and syntactic progressions, but once we're at the category level, most of the phonetic stuff takes place within the category; instead, we want to look at just the syntactic stuff.  Fortunately, PCA basis makes that relatively easy.  Pick a non-PCA1 basis component and sort by it -- PCA2 is usually long-range/key stats, while PCA3 is often the next ``syntactic" position.  Traditional transition stats can be extracted from that.  Are there any good examples?  If not, make this really short.

%chart: top PCA3-score destination categories for some prominent origin chords
