\chapter{Progressions}

Figure~\ref{iiprog} presents one of the most common truisms circulated regarding jazz harmony in a new form.  Each of the curves plotted here describes the statistical behavior of a chord category following the appearance of a locally-transposed $ii$ chord.\footnote{For details regarding the key-finding and local transposition, see Chapter 2.}  The details of Figure~\ref{iiprog} will emerge below, but the general form of the plot is simple: the vertical axis gives a measure of probability, while the horizontal displays time elapsed after a $ii$ chord.  Each curve tracks the probability of a particular chord appearing after $ii$ over time.

\begin{figure}
	\caption{The unigram-relative log probabilities of $ii$ (red line), $V$ (blue line), and $I$ (gray line) versus time (in milliseconds) elapsed after a $ii$ chord in the YJaMP corpus.  Here, the categories for $V$, $ii$, and $I$ have been hand-chosen; Chapter 4 provides a means to bootstrap category formation.}
	\label{iiprog}
	\centering
	\includegraphics[width=5.8in]{iiprogressions.png}
\end{figure}

The truism: in jazz, $ii$ often progresses to $V$ and then to $I$.  The curves of Figure~\ref{iiprog} have different shapes -- different behaviors over time -- which might be said to demonstrate and destabilize the truism in and with explicitly temporal terms.  The (blue, dashed) curve for $V$ rises to a probability peak between time points 500 and 2000 milliseconds, after which it slowly decreases in probability.  The (gray, dotted) curve for $I$ starts out less probable than $V$, but it becomes more probable as time elapses, surpassing $V$ around 2000ms.  Cast in traditional terms, these curves imply that when a $ii$ chord appears, it is likely that a $V$ chord appears some short time later, while a $I$ chord will probably appear some time after that.  This representation avoids bigram-style transitions between immediately adjacent states, instead displaying probabilistic tendencies over time.

The (red, solid) curve corresponding to $ii$ on Figure~\ref{iiprog} seems superfluous with respect to the truism above.  It shows that at very short times after the appearance of a $ii$ chord, another (or perhaps the same) $ii$ chord is highly probable.  When compared to the other curves on the plot, the $ii$ curve shows a very different shape and behavior.  I will return to this and the details of Figure~\ref{iiprog} in the following sections, but for now it suffices to note that this plot encapsulates at least two different scales of behavior: local reappearance of $ii$ and ``delayed" (however minimally) appearance of $V$ and $I$.  These modes of progression place it into dialog with schematic diagrams like Figure~\ref{pers}, taken from the harmonic progression work of Christopher Wm. White and Ian Quinn.\footnote{Christopher Wm.\ White and Ian Quinn (forthcoming). "Chord Context and Harmonic Function in Tonal Music."}

\begin{figure}
	\centering
	\includegraphics[width=5.5in]{WhiteQuinn_NewPersimmon.png}
	\caption{An empirically-derived ``persimmon" model of functional progression in the Bach chorales, taken from White and Quinn (2016).  Dotted lines indicate chord membership within functional categories.  Solid arrows indicate category-level progressions, whether function-preserving (self-progressions) or not (syntactic progressions).  While this diagram encodes progressions on different time scales, neither the progressions nor the functional categories engage with the temporal structure of the data.}
	\label{pers}
\end{figure}

On its most basic level, the ``persimmon" of Figure~\ref{pers} includes a combination of pitch-based chord labels (roman numerals) and functional categories ($T$, $S$, $D$, $P$, and $R$) linked by a variety of arrows.  For each functional category, the dotted lines on the figure provide a mapping indicating the kinds of chords likely to appear as members of the category.  Between categories themselves, solid arrows indicate common functional progressions.  Some progressions are from one syntactic function to another -- say, $T \rightarrow P$ or $P \rightarrow D$ -- while others indicate ``self-progressions" like $T \rightarrow T^x \rightarrow T$.  Each arrow thus corresponds to a chord transition, an immediately-adjacent bigram, and the actual pitch content of such a transition can be drawn from the pie-chart distributions relevant to each category.

Figure~\ref{iiprog} shows something similar to the solid, functional progression arrows on the persimmon, though it replaces the bigram orientation with a more explicit engagement with time.  While Figure~\ref{pers} indicates that $ii^7$ as predominant (P) frequently progresses to $V$ as dominant (D), Figure~\ref{iiprog} indicates temporal limits to such a claim; at very short and very long time scales, the $ii$ chords captured by Figure~\ref{iiprog} hardly progress to $V$ in any significant way, and it is only in a certain temporal regime that the $ii \rightarrow V$ syntactic progression claim seems to live.

This chapter provides a case study exposing the machinery behind and implications of Figure~\ref{iiprog} and its ability to describe multiple \emph{temporal progression regimes} within a single statistical framework.  Examples of short-timespan self-progressions (like the red $ii$ curve) and moderate-timespan syntactic progressions (like the blue $V$ curve) motivate the automated extraction of chords to which $ii$ tends to ``progress" at different time scales.  Principal component analysis (PCA), a frequent tool in computational time series analysis, will aid in reducing the temporal complexity of the many possible chords succeeding $ii$ at many possible scales.  A consideration of the most common ways $ii$ chords progress in time will suggest the possibility of ``bootstrapping" our types of progression, unearthing different temporal progression regimes directly from the statistical properties of YJaMP's solo piano performances.\footnote{The use of ``bootstrap" here draws on a long computational tradition of responding to a clear problem: if programmatic algorithms generate or load outputs from a series of inputs, how can a system receive the initial impetus to get off the ground?  (For early use of the term in this manner, see Buchholz, Werner (1953). ``The System Design of the IBM Type 701 Computer." Proceedings of the I.R.E. 41 (10): 1273. doi:10.1109/jrproc.1953.274300.)  I hand-supply plausible chord categories here, but I intend this as an initial approximation to be modified subsequently by machine learning methods.}  The case study here serves to anchor chord progressions in a temporal framework amenable to computation, and its methods apply to any scale-degree set in the corpus.  As Chapter 4 will indicate, the resulting temporal representation both captures nuanced progression behavior for individual chords and permits the construction of syntactic categories via elementary machine learning.  In the Kockelmanian terms of Chapter 2, the algorithms of this chapter turn scale-degree set signed indices into interpretants well-suited to the algorithmic agents of Chapter 4.

\section{Chords and Time}
%Chord ``behavior" different on different time scales
%Smallest: static, mostly self-transitions, lends itself to phonetic models
%Small: local behavior, mostly voice-leading neighbors
%Middle: ``Syntactic" models apply, here; suppressions (V-IV) and correlations (V-I)
%Large: predictive power recedes to background statistics (X-...-I)

Temporality and probability lie at the heart of Figure~\ref{iiprog}'s presentation of harmonic progressions.  These two concepts are concretized by the introduction of particular harmonic objects.  In the previous section, the term ``chord category" (applied to $ii$, $V$, and $I$) reflects an artificial (but rather conventional) definition of $ii$ as a category which includes the locally-transposed scale-degree sets $[2,5,9]$, $[0,2,5,9]$, and $[0,2,5]$.\footnote{In conventional scale degree terms, these chords are $(\hat{2},\hat{4},\hat{6})$, $(\hat{1},\hat{2},\hat{4}, \hat{6})$, and $(\hat{1},\hat{2},\hat{4})$.  For further details on the production of YJaMP scale-degree sets, see Chapter 2.}  As seen in Chapter 2, these scale degree sets represent the three most common instantiations of western-classical $ii$ chords produced via algorithmic parsing of the YJaMP corpus.  Along similar lines, the $V$ and $I$ categories indicated on Figure~\ref{iiprog} contain ($[2,7,11]$, $[2,5,7,11]$, $[5,7,11]$) and ($[0,4,7]$, $[0,3,7]$, $[0,4,11]$, $[0,3,10]$, $[0,4,7,11]$, $[0,3,7,10]$), respectively.  Machine learning methods will replace these chord category assumptions in Chapter 4, but they appear here as the simplest way to connect concepts from traditional harmony to computational methods.

The horizontal axis of Figure~\ref{iiprog} represents time in consecutive 50-millisecond windows ($w$) -- that is, $w=0$ is the time at which a $ii$ chord appears, `$w=1$ is 50 milliseconds later, and $w=50$ is 2.5 seconds (2500ms) after the initial $ii$ chord.  Each point on a plotted curve indicates how much more or less likely the curve's chord is to be found $w$ windows after a $ii$ chord than it is to be found in the corpus in general.  A Python algorithm finds every instance of a $ii$ chord in the corpus and tracks the time delay (in windows $w$) with which each chord appears in the subsequent 5 seconds.  A kind of composite statistical template results: given $ii$ as an ``origin chord," the algorithm returns a probability distribution for each window $w$ in which it accounts for every ``destination chord" found $50w$ milliseconds after a $ii$ chord.  These probability distributions consist of coded versions of claims like ``$w$ windows after a $ii$ chord, chord $x$ appears $y$ orders of magnitude more often than it does in the corpus generally."  

This paraphrased claim does not reference the absolute probability of appearance for the ``destination" chord $x$, since such a statistic would not make plain the predictive power of $ii$ itself.  A chord highly probable (in absolute terms) in a window $w$ might reflect a high background \emph{unigram} probability independent of the nearby presence of $ii$.\footnote{For further details on the quasi-Zipfian unigram distribution of the YJaMP corpus, see Chapter 2.}  Figure~\ref{iiprog} normalizes these distributions, however, plotting the difference between two logarithmic probabilities on the vertical axis:
\begin{equation}
P(x) = \log_{10} \big( p(x\mid ii) \big) - \log_{10} \big( p(x) \big)
\end{equation}
Here, $P(x)$ is the relative log probability of chord $x$ plotted on the vertical axis of Figure~\ref{iiprog}, while $p(x)$ and $p(x\mid ii)$ are the probabilities of $x$ in the corpus at large and given the previous occurrence of $ii$, respectively.  Subtracting these logarithmic probabilities is equivalent to dividing the probability-given-$ii$ by the probability-in-general outside the logarithmic domain.  The result of the logarithmic normalization is that the positive vertical axis values of Figure~\ref{iiprog} indicate probabilities higher than the background unigram distribution, while negative axis values indicate that the given chord occurs less frequently at a given distance $w$ after $ii$ than it does in the corpus in general -- that the chord is suppressed by $ii$.  Each increase of one unit on the vertical axis corresponds to a particular chord occurring 10 times more probably at a given time window $w$ than its unigram probability.

%talk about using these curves as templates for isolating types of progression behavior into different temporal regimes.
If the curves of Figure~\ref{iiprog} show the relative probabilities of three destination chords, they stand as a way of quantifying the effect of the origin chord ($ii$) on the presence of subsequent -- and not necessarily adjacent -- $ii$, $V$, and $I$ chords.  The appearance of $ii$ has one particular kind of effect at short distances -- it makes other $ii$ chords much more probable.  But how many chords does $ii$ impact in a similar way, and how many kinds of effect does $ii$ have?

\begin{figure}[h]
	\centering
	\caption{An excerpt of the temporal probability distributions for common chords following $ii$ (unigram-relative log probabilities versus time).  Traditional harmonic theory assumes the ability to reduce this complex temporal data to time-independent category transition rules.}
	\includegraphics[width=6in]{ii_messy.png}
	\label{ii_messy}
\end{figure}

Figure~\ref{ii_messy} presents a small fraction of the destination chord probability distributions for chords following $ii$.  For many high-probability origin chord types in the corpus (like $ii$), tokens corresponding to more than 1000 distinct destination chord types appear within the 5 seconds following all of the origin chord's tokens across the corpus.  Sorting through these distributions by hand is not feasible, and many of the destination chords are low-probability objects in the first place; there is no guarantee that random fluctuations in the appearance of rare chords will tell us anything about $ii$'s syntactic properties.

Thankfully, to find other chords strongly impacted by $ii$ in ways similar to the behavior displayed in Figure~\ref{iiprog}, it suffices to compare the large number of destination chord probability distributions to the (red, blue, and gray) templates provided by $ii$, $V$, and $I$.  Curves which lie close to the $V$ curve correspond to chords with temporal behavior similar to $V$ in the context of $ii$.  This sounds tautological, but an important distinction is at play: a chord with a temporal probability distribution similar to that of $V$ on Figure~\ref{iiprog} may have absolutely no discernible pitch relation to $V$ as a scale degree set.  ``$V$-like" chords, distributionally speaking, need only \emph{act like} $V$, statistically, in the presence of $ii$.  In other words, the algorithmic agent calculating which destination chord probability distributions lie closest to those shown in Figure~\ref{iiprog} picks out chords which display temporal behavior similar to self-progressions and syntactic progressions.  This readily automatable process assembles chord progression categories on more than one time scale -- and it can be performed for any origin chord.

\section{Phonetic scale}
On the shortest of musical time scales, the $ii$ chord tends to predict itself quite strongly, and it does so with diminishing strength as time passes.  To find what other chords behave this way after the appearance of $ii$, a few lines of code iterate across all of the observed destination chord distributions.  For each destination chord $d$, the code sees an $n$-dimensional probability vector, where $n$ can range up to the value of $w$ corresponding to the maximum time delay for which statistics are kept.  It compares this vector to the $n$-dimensional ``self-progression" prototype (the $ii$ distribution) element-wise, subtracting the log probability of the current destination chord ($p_d$) from the log probability of the $ii$ chord ($p_{ii}$) at each window location.  Summing the absolute value of all of these deviations gives a simple metric for how dissimilar the destination chord is from $ii$:
\begin{eqnarray}
p_{ii} &=& (x_1, x_2, ... ,  x_n) \\
p_{d} &=& (y_1, y_2, ..., y_n) \\
D_{(ii,d)} &=& \sum_{w=1}^{n} \lvert y_w - x_w \rvert
\end{eqnarray}
That is, the code subtracts the probability of the origin chord $ii$ in window 1 from the probability of the destination chord $d$ in window 1, takes the absolute value of the difference, and combines the resulting distances over some $n$ consecutive time windows.  Setting $n$ is a matter of taste; I have chosen here to compare curves out to where the behavior of the prototype begins to level off.  For the self-progression $ii \rightarrow ii$, Figure~\ref{iiprog} suggests $n=40$ as a reasonable cutoff, though employing similar values yields similar results.  Minimizing dissimilarity $D_{ii,d}$ provides a list of the destination chords which best fit the short-term behavior of $ii$.  A plot of the 5 best destination chords of this type is given in Figure~\ref{ii_phonetic}.

\begin{figure}
	\centering
	\caption{The 5 destination chords for $ii$ progressions with temporal probability distributions most similar to that of $ii$ (unigram-relative log probabilities versus time).}
	\includegraphics[width=6in]{ii_phonetic.png}
	\label{ii_phonetic}
\end{figure}

The results are striking in their predictability.  While pitch class/scale degree content played no role in the selection of self-progression destinations for $ii$, most of the destination chords with behavior most similar to $ii$ consist of some form of $ii$ chord with one or more added tones. $[0,2,3,5,9]$ and $[2,4,5,9]$ likely draw scale tones from their minor and major tonics, respectively, while $[0,1,2,5]$ and $[2,3,4,5,9]$ appear to arise from some local melodic motion.  In general, the list of self-progression partners for $ii$ suggests two rough categories of behavior:
\begin{enumerate}
	\item The destination chord may be related to the origin $ii$ chord by harmonic or quality completion.  Example: $[0,2,5] \rightarrow [0,2,5,9]$.
	\item The destination chord may be related to the origin $ii$ chord by the introduction of melodic notes. Example: $[0,2,5,9] \rightarrow [0,2,3,5,9]$.
\end{enumerate}
The two types of behavior can be combined, and there are also conditions where a self-progression may ambiguously result from either (i.e., $[2,5,9] \rightarrow [0,2,5,9]$).  More detailed statistics regarding the particular scale degree sets and voicings participating in these progressions can be gleaned from combining these observation with the unigram-labeling methods of Chapter 2.

In general, few of the progressions at this time scale consist of transitions which an analyst would deem sufficient to indicate a change of ``chord" or ``function."  Moreover, several self-progressions consecutively might be necessary to fill out all the pitches implied by an analyst's roman numeral.  As a result, I refer to this temporal progression regime as the \emph{phonetic regime}, by way of analogy to linguistics.  In the way that multiple phonemes might make up a word, multiple self-progressions in this regime might make up a functional chord, and the particular ways in which phonetic regime chords are connected to one another rely on particular choices of voicing and melodic context in much the same way that the surface-level phonetic traces of linguistic syntax depend on the particulars of word order and nearby syntactic partners.\footnote{I feel certain that the phonetic analogy with musical chord fragments does not originate with me, but I have yet to find a smoking gun.  Until someone enlightens me, I will continue hunting for throwaway comments made by Ray Jackendoff.}  If harmonic syntax is to include primarily progressions from one function to another, then the phonetic regime provides a clean and empirically-grounded sense of the morphological transformations necessitated by concatenating syntactic partners.

So far, I have avoided mentioning the fifth most $ii$-like member of the (fuzzy) phonetic progression regime, shown in Figure~\ref{ii_phonetic}: $[2,5,11]$.  This chord, usually labeled $vii^{\circ}$, is not typically said to have the same function as $ii$.  Its presence on the plot might imply two things: that $[2,5,11]$ may appear as a phonetic neighbor to a more normative $ii$ chord (like $[2,5,9]$), or that destination chord curves this far below the self-progression $ii$ prototype begin to behave more like syntactic progression destinations than phonetic ones.  The $[2,5,11]$ distribution of Figure~\ref{ii_phonetic} falls almost exactly midway between the prototype curves for $ii$ and $V$ on Figure~\ref{iiprog}, and its behavior in different contexts might suggest assigning it a dual status, playing a role in the phonetic and syntactic regimes.

In any event, chord category formation involves much more than a quick perusal of Figure~\ref{ii_phonetic}.  Comparing destination chord regimes across many origin chords provides a much broader data set better suited to machine learning techniques.  Example phonetic and syntactic regime plots for origin chords other than $ii$ are included at the end of this chapter, and the integration of these disparate distributional portraits with machine learning is left to the next chapter.

\section{Local syntactic scale}
Finding syntactic progressions amounts to a change in time scale.  The previous section showed that self-progression destination chords could be found by calculating which distributions most closely resembled $ii\rightarrow ii$ over short time scales, regardless of pitch content.  The same logic applies to functional syntactic progressions: if $ii\rightarrow V$ is a prototypical syntactic progression, syntactic regime destination chords should have temporal probability distributions similar to that of $V$ over moderate time scales.  Judging from Figure~\ref{iiprog}, that time scale is about $60$ time windows, or 3 seconds.\footnote{Here, again, I choose the time scale cutoff rather intuitively, assuming that the initial suppression, sharp increase in probability, and return to unigram probability are all features of syntactic-$V$-ness.  It is unclear to me, however, if the lower-probability fluctuations at the end of the temporal tail bear important information.}  The best-fitting curves appear in Figure~\ref{ii_syntactic}.

\begin{figure}
	\centering
	\includegraphics[width=6in]{ii_syntactic.png}
	\caption{The 7 destination chords following $ii$ with syntactic regime probability distributions most closely resembling that of $V$.  Chords similar to both $V$ and $IV$ appear to behave similarly.}
	\label{ii_syntactic}
\end{figure}

Many of the destination chords which most closely resemble $V$ in this context are chords we might think of as phonetically-related to $V$ proper.  $[1,3,5,7,11]$ can be parsed as a $V^{\sharp 11 \flat 13}$, and $[2,5,7]$ may function as an incomplete $V^7$.  $[2,7,9]$ might be related to $[2,7,11]$ by melodic step, or it could be a symptom of more complicated behavior in play; $[2,4,7,8,10,11]$ is almost certainly the latter.  But the syntactic nature of the $V$ curve does more than pick out its own phonetic neighbors.  Since no pitch class or root similarities are assumed, behavioral similarity also strongly selects another type of chord likely to follow $ii$: $IV$ chords.  Shown in the dotted curves of Figure~\ref{ii_syntactic}, $[0,4,5,9]$ and $[0,5,7,9]$ are both likely to follow $ii$ in a way similar to $V$ -- both types of chord would be deemed syntactic if found a moderate time after $ii$.

In this way, syntactic regime progression data partly reproduces and partly cuts across traditional functional harmony expectations.  Crucially, not all chords which behave like $V$ after $ii$ \emph{always} behave like $V$.  Each possible origin chord provides a distinct set of phonetic and syntactic data to be used for functional categorization.  The $ii$ syntactic data here indicates a category of ``chords which follow $ii$ at moderate delays," of which both $IV$ and $V$ are members, but other syntactic data might distinguish the two.  Before Chapter 5 attempts to address this inductive category formation problem, it is worth asking another difficult inductive question directly: how many temporal syntactic regimes are there, and can they be extracted from the data without assuming prior prototypes?

\section{Temporal Progression Regimes}
The selection of $ii$, $V$, and $I$ as prototypes for categories of temporal progression behavior served an important purpose.  Since probabilistic statistics connect each origin chord to a large number of destination chords, some principled method for selecting destination chords with common behaviors is necessary.  The examples used in this chapter depend on thousands of 100-dimensional vectors, each of which features its own unique shape.  Choosing temporal progression prototypes constitutes a selection of shape types.  I assumed that the shapes of the $ii$, $V$, and $I$ chord distributions (when following a $ii$ origin chord) were common and important ones.  But the data can suggest its own collection of shape types.

Principal component analysis (PCA) treats problems of exactly this nature in the world of machine learning.\footnote{Common in the machine learning literature, PCA first appears in Pearson, K. (1901). "On Lines and Planes of Closest Fit to Systems of Points in Space." \emph{Philosophical Magazine} 2 (11): 559–572.  Other machine learning techniques accomplish similar ends, and I have no proof that PCA provides optimal results -- but it does provide results.}  Given a data set with a high-dimensional coordinate basis -- like a set of 100-dimensional vectors -- PCA constructs a lower-dimensional coordinate system, projecting elements of the data set onto new basis vectors which best account for a maximum amount of the variance in the data.  The details are complex, but some broad-strokes intuitions help contextualize the results of interest.

To cast the destination chord probability distributions into a form amenable to PCA, it helps to think of each distribution as a single point in a plot with 100 axes -- one corresponding to each time window. In schematic form, PCA then asks a series of questions about this noisy cloud of temporal probability distribution points:
\begin{enumerate}
	\item How do the data points (destination chord distributions) vary with respect to each of the 100 (temporal) basis coordinates?
	\item Are the distributions at any combination of axes (that is, time coordinates) correlated with one another?
	\item Along what direction in this 100-dimensional space is the largest variance in the data?  If a non-trivial set of time coordinates frequently indexes correlated probability values, that direction may not be along a single one of the 100 axes.
	\item Can a small number of such directions reasonably capture the majority of the variance in the data?  If so, re-label the data in terms of those directions, rather than the original 100 axes.
\end{enumerate}

The first two questions ask if the behavior of destination chords at, say, 50 milliseconds tends to be correlated with their behavior at some other time(s).  Most of the temporal probability distributions extracted from YJaMP are quasi-continuous -- the probability of $V$ at time point 4 is likely to be close to the probability of $V$ at time point 3 -- which means that consecutive time points tend to index similar probability values for a given destination chord distribution.  In the ``cloud of points" frame of reference, this means that a distribution with a high value along axis 4 is relatively likely to have a similarly high value along axes 3 and 5. The probability at consecutive time points is likely to be positively correlated.  Alternately, destination chord distributions with high probability values at very short distances may tend to have low probabilities at long distances, or vice versa, producing negative correlation.  In either case, correlated temporal probability data exists in the distributions over their time coordinate basis.

The third question asks if a particular direction in the cloud of points can capture a large swath of the data.  If the destination chord probability distributions were identical at all time scales except for the very first window, this question would be trivial: the direction of the first axis would account for \emph{all} of the variance in the data, and a new set of coordinates could discard 99 of the axis variables without losing any information about differences in the distributions.  But for the actual YJaMP progression data, the distributions usually vary over all 100 time scales, many of which are non-trivially correlated.  It might be that chords in the phonetic regime primarily vary over windows 1-20, and that the variance in each of those windows is strongly correlated.  If those variances are similar, a PCA algorithm will select as a basis vector some linear combination of the original axes that is maximally aligned with the direction of variance.

The fourth question ensures that the variance-maximizing basis vectors are produced in descending order of variance capture.  Each of the first few newly-calculated axes -- principal components -- can be used to describe a large amount of the data. Crucially, after the first few components, PCA enables the analyst to stop tracking dimensions while losing a minimal amount of information regarding the differences between the data points.  Instead of searching for patterns in all 100 time variables of the figures above, PCA focuses on a small number of new dimensions.

All of these machinations produce two positive outcomes: First, these new components form a coordinate system in which the destination chord probability distributions have radically simpler coordinates -- only a few numbers, instead of 100.  And secondly, the resulting components provide templates for the ways in which categories of destination chord distribution differ from each other most significantly.  They provide automated, data-driven indications as to how many different temporal progression regimes there might be, and they present shape types indicative of the types of variance to expect.

Running PCA on the large number of destination chord probability distributions decomposes the high-dimensional time-based coordinates into a low-dimensional basis of temporal progression regimes.  The first 5 machine-generated principal components for $ii$ chord destinations are plotted in Figure~\ref{ii_pca}.
\begin{figure}
	\centering
	\caption{The top 5 principal components calculated from the full set of destination chord temporal probability distributions following $ii$.  The plot is of probability versus time; the legend indicates the components and their variance percentages in descending order.}
	\includegraphics[width=6in]{iipca_ABS.png}
	\label{ii_pca}
\end{figure}

With slight theoretical adjustments, I propose that these distribution shape classes (or, more properly, combinations thereof) can be used in place of the hand-picked $ii$, $V$, and $I$ shape templates employed earlier in the chapter for the assembly of phonetic and syntactic progression regimes.  The first principal component, the blue curve on the figure, accounts for most of the variance in the distributions; high and low scores with respect to this component indicate behavior where the shorter time spans vary much more than the longer ones, behavior comparable to that of the phonetic regime.  Component 2, the green curve, corresponds to chords initially suppressed but predicted later, similar to the behavior of chords in the syntactic regime.  Components 3-5 indicate more nuanced variance behavior, but they only account for a very small percentage of the data's variance: the lion's share of information is captured by the first two components, providing strong motivation for the existence of precisely two progression regimes -- the phonetic and the syntactic.

These machine-generated regimes of chord behavior provide a framework subsuming both ``embellishing" and ``harmonic" progressions -- a general statistical model of temporal progression behavior.  They rely on no particular knowledge regarding the pitch content of the chords involved, and they suggest both the number of regimes and their characteristic shapes empirically.  They provide a proof of concept that progression types for a given origin chord (in this case, $ii$) can be learned from complex data with minimal assumptions.  Extending this information to a variety of origin chords and inducing a grammar through machine learning are left to the next chapter.

\newpage
\section{Other Origin Chords}

%dump, for now
\begin{figure}[h]
	\centering
	\includegraphics[width=6in]{I_phonetic.png}
	\caption{Phonetic regime self-progression destination chords following the appearance of $I$.}
	\label{I_phonetic}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=6in]{V_phonetic.png}
	\caption{Phonetic regime self-progression destination chords following the appearance of $V$.}
	\label{V_phonetic}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=6in]{V_syntactic.png}
	\caption{Syntactic regime destination chords following the appearance of $V$.  Note the comparatively short time delay; $V$ does not have a strong predictive impact on the appearance of $I$ for very long.  This is likely due both to the very high unigram probabilities of $I$ and $V$ and to $V$'s role in faster progressions (compared to $ii$).}
	\label{V_syntactic}
\end{figure}
